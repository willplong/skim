{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from jax import vmap\n",
    "from jax.lib import xla_bridge\n",
    "from jax.scipy.linalg import cho_factor, cho_solve, solve_triangular\n",
    "from numpyro.infer import MCMC, NUTS\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Sparse Regression Model\n",
    "\n",
    "We demonstrate how to do (fully Bayesian) sparse linear regression using the approach described in\n",
    "[1]. This approach is particularly suitable for situations with many feature dimensions (large P)\n",
    "but not too many data points (small N).\n",
    "\n",
    "In particular, we consider a quadratic regressor of the form:\n",
    "\n",
    "$$f(X) = \\text{constant} + \\sum_i \\theta_i X_i + \\sum_{i<j} \\theta_{ij} X_i X_j + \\text{observation noise}$$\n",
    "    \n",
    "**References:**\n",
    "1. Raj Agrawal, Jonathan H. Huggins, Brian Trippe, Tamara Broderick (2019), \"The Kernel Interaction\n",
    "   Trick: Fast Bayesian Discovery of Pairwise Interactions in High Dimensions\",\n",
    "   (https://arxiv.org/abs/1905.06501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(X, Z):\n",
    "    return jnp.dot(X, Z[..., None])[..., 0]\n",
    "\n",
    "\n",
    "# The kernel that corresponds to our quadratic regressor.\n",
    "def kernel(X, Z, eta1, eta2, c, jitter=1.0e-4):\n",
    "    eta1sq = jnp.square(eta1)\n",
    "    eta2sq = jnp.square(eta2)\n",
    "    k1 = 0.5 * eta2sq * jnp.square(1.0 + dot(X, Z))\n",
    "    k2 = -0.5 * eta2sq * dot(jnp.square(X), jnp.square(Z))\n",
    "    k3 = (eta1sq - eta2sq) * dot(X, Z)\n",
    "    k4 = jnp.square(c) - 0.5 * eta2sq\n",
    "    if X.shape == Z.shape:\n",
    "        k4 += jitter * jnp.eye(X.shape[0])\n",
    "    return k1 + k2 + k3 + k4\n",
    "\n",
    "\n",
    "# Most of the model code is concerned with constructing the sparsity inducing prior.\n",
    "def model(X, Y, hypers):\n",
    "    S, P, N = hypers[\"expected_sparsity\"], X.shape[1], X.shape[0]\n",
    "\n",
    "    sigma = numpyro.sample(\"sigma\", dist.HalfNormal(hypers[\"alpha3\"]))\n",
    "    phi = sigma * (S / jnp.sqrt(N)) / (P - S)\n",
    "    eta1 = numpyro.sample(\"eta1\", dist.HalfCauchy(phi))\n",
    "\n",
    "    msq = numpyro.sample(\"msq\", dist.InverseGamma(hypers[\"alpha1\"], hypers[\"beta1\"]))\n",
    "    xisq = numpyro.sample(\"xisq\", dist.InverseGamma(hypers[\"alpha2\"], hypers[\"beta2\"]))\n",
    "\n",
    "    eta2 = jnp.square(eta1) * jnp.sqrt(xisq) / msq\n",
    "\n",
    "    lam = numpyro.sample(\"lambda\", dist.HalfCauchy(jnp.ones(P)))\n",
    "    kappa = jnp.sqrt(msq) * lam / jnp.sqrt(msq + jnp.square(eta1 * lam))\n",
    "\n",
    "    # compute kernel\n",
    "    kX = kappa * X\n",
    "    k = kernel(kX, kX, eta1, eta2, hypers[\"c\"]) + sigma**2 * jnp.eye(N)\n",
    "    assert k.shape == (N, N)\n",
    "\n",
    "    # sample Y according to the standard gaussian process formula\n",
    "    numpyro.sample(\n",
    "        \"Y\",\n",
    "        dist.MultivariateNormal(loc=jnp.zeros(X.shape[0]), covariance_matrix=k),\n",
    "        obs=Y,\n",
    "    )\n",
    "\n",
    "# Helper function for doing HMC inference\n",
    "def run_inference(model, rng_key, X, Y, hypers, num_warmup, num_samples, num_chains):\n",
    "    start = time.time()\n",
    "    kernel = NUTS(model)\n",
    "    mcmc = MCMC(\n",
    "        kernel,\n",
    "        num_warmup=num_warmup,\n",
    "        num_samples=num_samples,\n",
    "        num_chains=num_chains,\n",
    "        progress_bar=False if \"NUMPYRO_SPHINXBUILD\" in os.environ else True,\n",
    "    )\n",
    "    mcmc.run(rng_key, X, Y, hypers)\n",
    "    print(\"\\nMCMC elapsed time:\", time.time() - start)\n",
    "    return mcmc.get_samples()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_relevant_chromosome(chromosome):\n",
    "    # Try running on all chromosomes, but uncomment other line if runtime is too long.\n",
    "    return True\n",
    "    # return chromosome in [\"1\", \"14\", \"17\", \"21\"]\n",
    "\n",
    "donor_braak_dict = {}\n",
    "with open(\"data/donor-info.csv\", \"r\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        donor_braak_dict[row[\"donor_id\"]] = int(row[\"braak\"])\n",
    "\n",
    "rnaseq_donor_dict = {}\n",
    "with open(\"data/columns-samples.csv\", \"r\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        rnaseq_donor_dict[row[\"rnaseq_profile_id\"]] = row[\"donor_id\"]\n",
    "\n",
    "gene_chromosome_dict = {}\n",
    "gene_symbol_dict = {}\n",
    "with open(\"data/rows-genes.csv\", \"r\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        gene_chromosome_dict[row[\"gene_id\"]] = row[\"chromosome\"]\n",
    "        gene_symbol_dict[row[\"gene_id\"]] = row[\"gene_symbol\"]\n",
    "\n",
    "with open(\"data/fpkm_table_normalized.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)\n",
    "    rnaseq_profiles = header[1:]\n",
    "    donors = [rnaseq_donor_dict[profile] for profile in rnaseq_profiles]\n",
    "    braak_scores = [donor_braak_dict[donor] for donor in donors]\n",
    "    Y = np.array(braak_scores)\n",
    "\n",
    "    X = []\n",
    "    gene_symbols = []\n",
    "    for row in reader:\n",
    "        gene_id = row[0]\n",
    "        chromosome = gene_chromosome_dict[gene_id]\n",
    "        if is_relevant_chromosome(chromosome):\n",
    "            gene_symbols.append(gene_symbol_dict[gene_id])\n",
    "            fpkms = [float(val) for val in row[1:]]\n",
    "            X.append(fpkms)\n",
    "    gene_symbols = np.array(gene_symbols)\n",
    "    X = np.array(X).T\n",
    "\n",
    "print(\"Shape of X: \", X.shape)\n",
    "print(\"Shape of Y: \", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_thresh = 0.05\n",
    "num_select = 100\n",
    "\n",
    "variances = np.var(X, axis=0)\n",
    "print(f\"# Genes with Variance > {var_thresh}: \", np.sum(variances > var_thresh))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.hist(variances, bins=np.linspace(0, 1, 100))\n",
    "ax.axvline(var_thresh, color=\"red\", linestyle=\"--\")\n",
    "ax.set_xlabel(\"Variance\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_selector = VarianceThreshold(var_thresh)\n",
    "X = variance_selector.fit_transform(X)\n",
    "selected_indices = variance_selector.get_support(indices=True)\n",
    "gene_symbols = gene_symbols[selected_indices]\n",
    "\n",
    "# TODO: Should we be using a different feature selection method?\n",
    "select_k_best = SelectKBest(k=num_select)\n",
    "X = select_k_best.fit_transform(X, Y)\n",
    "selected_indices = select_k_best.get_support(indices=True)\n",
    "gene_symbols = gene_symbols[selected_indices]\n",
    "\n",
    "print(\"Shape of X: \", X.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = X.shape[0]\n",
    "num_dimensions = X.shape[1]\n",
    "num_active = 10\n",
    "num_samples = 1000\n",
    "num_warmup = 500\n",
    "device = xla_bridge.get_backend().platform\n",
    "num_chains = jax.device_count() if device == \"gpu\" else 1\n",
    "\n",
    "print(f\"Running {num_chains} chains on {device}\")\n",
    "numpyro.set_host_device_count(num_chains)\n",
    "numpyro.set_platform(device)\n",
    "\n",
    "hypers = {\n",
    "    \"expected_sparsity\": num_active,\n",
    "    \"alpha1\": 3.0,\n",
    "    \"beta1\": 1.0,\n",
    "    \"alpha2\": 3.0,\n",
    "    \"beta2\": 1.0,\n",
    "    \"alpha3\": 1.0,\n",
    "    \"c\": 1.0,\n",
    "}\n",
    "\n",
    "rng_key = random.PRNGKey(0)\n",
    "samples = run_inference(\n",
    "    model, rng_key, X, Y, hypers, num_warmup, num_samples, num_chains\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.datetime.now().strftime(\"%m%d-%H%M%S\")\n",
    "with open(f\"out/mcmc-{date}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(samples, f)\n",
    "with open(f\"out/genes-{date}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(gene_symbols, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean and variance of coefficient theta_i (where i = dimension) for a\n",
    "# MCMC sample of the kernel hyperparameters (eta1, xisq, ...).\n",
    "# Compare to theorem 5.1 in reference [1].\n",
    "def compute_singleton_mean_variance(X, Y, dimension, msq, lam, eta1, xisq, c, sigma):\n",
    "    P, N = X.shape[1], X.shape[0]\n",
    "\n",
    "    probe = jnp.zeros((2, P))\n",
    "    probe = probe.at[:, dimension].set(jnp.array([1.0, -1.0]))\n",
    "\n",
    "    eta2 = jnp.square(eta1) * jnp.sqrt(xisq) / msq\n",
    "    kappa = jnp.sqrt(msq) * lam / jnp.sqrt(msq + jnp.square(eta1 * lam))\n",
    "\n",
    "    kX = kappa * X\n",
    "    kprobe = kappa * probe\n",
    "\n",
    "    k_xx = kernel(kX, kX, eta1, eta2, c) + sigma**2 * jnp.eye(N)\n",
    "    k_xx_inv = jnp.linalg.inv(k_xx)\n",
    "    k_probeX = kernel(kprobe, kX, eta1, eta2, c)\n",
    "    k_prbprb = kernel(kprobe, kprobe, eta1, eta2, c)\n",
    "\n",
    "    vec = jnp.array([0.50, -0.50])\n",
    "    mu = jnp.matmul(k_probeX, jnp.matmul(k_xx_inv, Y))\n",
    "    mu = jnp.dot(mu, vec)\n",
    "\n",
    "    var = k_prbprb - jnp.matmul(k_probeX, jnp.matmul(k_xx_inv, jnp.transpose(k_probeX)))\n",
    "    var = jnp.matmul(var, vec)\n",
    "    var = jnp.dot(var, vec)\n",
    "\n",
    "    return mu, var\n",
    "\n",
    "\n",
    "# Compute the mean and variance of coefficient theta_ij for a MCMC sample of the\n",
    "# kernel hyperparameters (eta1, xisq, ...). Compare to theorem 5.1 in reference [1].\n",
    "def compute_pairwise_mean_variance(X, Y, dim1, dim2, msq, lam, eta1, xisq, c, sigma):\n",
    "    P, N = X.shape[1], X.shape[0]\n",
    "\n",
    "    probe = jnp.zeros((4, P))\n",
    "    probe = probe.at[:, dim1].set(jnp.array([1.0, 1.0, -1.0, -1.0]))\n",
    "    probe = probe.at[:, dim2].set(jnp.array([1.0, -1.0, 1.0, -1.0]))\n",
    "\n",
    "    eta2 = jnp.square(eta1) * jnp.sqrt(xisq) / msq\n",
    "    kappa = jnp.sqrt(msq) * lam / jnp.sqrt(msq + jnp.square(eta1 * lam))\n",
    "\n",
    "    kX = kappa * X\n",
    "    kprobe = kappa * probe\n",
    "\n",
    "    k_xx = kernel(kX, kX, eta1, eta2, c) + sigma**2 * jnp.eye(N)\n",
    "    k_xx_inv = jnp.linalg.inv(k_xx)\n",
    "    k_probeX = kernel(kprobe, kX, eta1, eta2, c)\n",
    "    k_prbprb = kernel(kprobe, kprobe, eta1, eta2, c)\n",
    "\n",
    "    vec = jnp.array([0.25, -0.25, -0.25, 0.25])\n",
    "    mu = jnp.matmul(k_probeX, jnp.matmul(k_xx_inv, Y))\n",
    "    mu = jnp.dot(mu, vec)\n",
    "\n",
    "    var = k_prbprb - jnp.matmul(k_probeX, jnp.matmul(k_xx_inv, jnp.transpose(k_probeX)))\n",
    "    var = jnp.matmul(var, vec)\n",
    "    var = jnp.dot(var, vec)\n",
    "\n",
    "    return mu, var\n",
    "\n",
    "\n",
    "# Sample coefficients theta from the posterior for a given MCMC sample.\n",
    "# The first P returned values are {theta_1, theta_2, ...., theta_P}, while\n",
    "# the remaining values are {theta_ij} for i,j in the list `active_dims`,\n",
    "# sorted so that i < j.\n",
    "def sample_theta_space(X, Y, active_dims, msq, lam, eta1, xisq, c, sigma):\n",
    "    P, N, M = X.shape[1], X.shape[0], len(active_dims)\n",
    "    # the total number of coefficients we return\n",
    "    num_coefficients = P + M * (M - 1) // 2\n",
    "\n",
    "    probe = jnp.zeros((2 * P + 2 * M * (M - 1), P))\n",
    "    vec = jnp.zeros((num_coefficients, 2 * P + 2 * M * (M - 1)))\n",
    "    start1 = 0\n",
    "    start2 = 0\n",
    "\n",
    "    for dim in range(P):\n",
    "        probe = probe.at[start1 : start1 + 2, dim].set(jnp.array([1.0, -1.0]))\n",
    "        vec = vec.at[start2, start1 : start1 + 2].set(jnp.array([0.5, -0.5]))\n",
    "        start1 += 2\n",
    "        start2 += 1\n",
    "\n",
    "    for dim1 in active_dims:\n",
    "        for dim2 in active_dims:\n",
    "            if dim1 >= dim2:\n",
    "                continue\n",
    "            probe = probe.at[start1 : start1 + 4, dim1].set(\n",
    "                jnp.array([1.0, 1.0, -1.0, -1.0])\n",
    "            )\n",
    "            probe = probe.at[start1 : start1 + 4, dim2].set(\n",
    "                jnp.array([1.0, -1.0, 1.0, -1.0])\n",
    "            )\n",
    "            vec = vec.at[start2, start1 : start1 + 4].set(\n",
    "                jnp.array([0.25, -0.25, -0.25, 0.25])\n",
    "            )\n",
    "            start1 += 4\n",
    "            start2 += 1\n",
    "\n",
    "    eta2 = jnp.square(eta1) * jnp.sqrt(xisq) / msq\n",
    "    kappa = jnp.sqrt(msq) * lam / jnp.sqrt(msq + jnp.square(eta1 * lam))\n",
    "\n",
    "    kX = kappa * X\n",
    "    kprobe = kappa * probe\n",
    "\n",
    "    k_xx = kernel(kX, kX, eta1, eta2, c) + sigma**2 * jnp.eye(N)\n",
    "    L = cho_factor(k_xx, lower=True)[0]\n",
    "    k_probeX = kernel(kprobe, kX, eta1, eta2, c)\n",
    "    k_prbprb = kernel(kprobe, kprobe, eta1, eta2, c)\n",
    "\n",
    "    mu = jnp.matmul(k_probeX, cho_solve((L, True), Y))\n",
    "    mu = jnp.sum(mu * vec, axis=-1)\n",
    "\n",
    "    Linv_k_probeX = solve_triangular(L, jnp.transpose(k_probeX), lower=True)\n",
    "    covar = k_prbprb - jnp.matmul(jnp.transpose(Linv_k_probeX), Linv_k_probeX)\n",
    "    covar = jnp.matmul(vec, jnp.matmul(covar, jnp.transpose(vec)))\n",
    "\n",
    "    # sample from N(mu, covar)\n",
    "    L = jnp.linalg.cholesky(covar)\n",
    "    sample = mu + jnp.matmul(L, np.random.randn(num_coefficients))\n",
    "\n",
    "    return sample\n",
    "\n",
    "\n",
    "# Get the mean and variance of a gaussian mixture\n",
    "def gaussian_mixture_stats(mus, variances):\n",
    "    mean_mu = jnp.mean(mus)\n",
    "    mean_var = jnp.mean(variances) + jnp.mean(jnp.square(mus)) - jnp.square(mean_mu)\n",
    "    return mean_mu, mean_var\n",
    "\n",
    "\n",
    "# Helper function for analyzing the posterior statistics for coefficient theta_i\n",
    "def analyze_dimension(samples, X, Y, dimension, hypers):\n",
    "    vmap_args = (\n",
    "        samples[\"msq\"],\n",
    "        samples[\"lambda\"],\n",
    "        samples[\"eta1\"],\n",
    "        samples[\"xisq\"],\n",
    "        samples[\"sigma\"],\n",
    "    )\n",
    "    mus, variances = vmap(\n",
    "        lambda msq, lam, eta1, xisq, sigma: compute_singleton_mean_variance(\n",
    "            X, Y, dimension, msq, lam, eta1, xisq, hypers[\"c\"], sigma\n",
    "        )\n",
    "    )(*vmap_args)\n",
    "    mean, variance = gaussian_mixture_stats(mus, variances)\n",
    "    std = jnp.sqrt(variance)\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "# Helper function for analyzing the posterior statistics for coefficient theta_ij\n",
    "def analyze_pair_of_dimensions(samples, X, Y, dim1, dim2, hypers):\n",
    "    vmap_args = (\n",
    "        samples[\"msq\"],\n",
    "        samples[\"lambda\"],\n",
    "        samples[\"eta1\"],\n",
    "        samples[\"xisq\"],\n",
    "        samples[\"sigma\"],\n",
    "    )\n",
    "    mus, variances = vmap(\n",
    "        lambda msq, lam, eta1, xisq, sigma: compute_pairwise_mean_variance(\n",
    "            X, Y, dim1, dim2, msq, lam, eta1, xisq, hypers[\"c\"], sigma\n",
    "        )\n",
    "    )(*vmap_args)\n",
    "    mean, variance = gaussian_mixture_stats(mus, variances)\n",
    "    std = jnp.sqrt(variance)\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mean and square root variance of each coefficient theta_i\n",
    "dims = jnp.arange(num_dimensions)\n",
    "means, stds = vmap(lambda dim: analyze_dimension(samples, X, Y, dim, hypers))(dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_dimensions = []\n",
    "for dim, (mean, std) in enumerate(zip(means, stds)):\n",
    "    # mark dimension inactive if interval [mean +/- 3 * std] contains zero\n",
    "    lower, upper = mean - 3.0 * std, mean + 3.0 * std\n",
    "    if lower * upper > 0.0:\n",
    "        active_dimensions.append(dim)\n",
    "        print(f\"theta[{dim + 1}]: {mean:.2e} +- {std:.2e}\")\n",
    "\n",
    "print(f\"Identified a total of {len(active_dimensions)} active dimensions\")\n",
    "\n",
    "# Compute the mean and square root variance of coefficients theta_ij for i,j active dimensions.\n",
    "# Note that the resulting numbers are only meaningful for i != j.\n",
    "if len(active_dimensions) > 0:\n",
    "    dim_pairs = jnp.array(list(itertools.product(active_dimensions, active_dimensions)))\n",
    "    means, stds = vmap(\n",
    "        lambda dim_pair: analyze_pair_of_dimensions(\n",
    "            samples, X, Y, dim_pair[0], dim_pair[1], hypers\n",
    "        )\n",
    "    )(dim_pairs)\n",
    "    for dim_pair, mean, std in zip(dim_pairs, means, stds):\n",
    "        dim1, dim2 = dim_pair\n",
    "        if dim1 >= dim2:\n",
    "            continue\n",
    "        lower, upper = mean - 3.0 * std, mean + 3.0 * std\n",
    "        if lower * upper > 0.0:\n",
    "            print(f\"theta[{dim1 + 1}, {dim2 + 1}]: {mean:.2e} +- {std:.2e}]\")\n",
    "\n",
    "    # Draw a single sample of coefficients theta from the posterior, where we return all singleton\n",
    "    # coefficients theta_i and pairwise coefficients theta_ij for i, j active dimensions. We use the\n",
    "    # final MCMC sample obtained from the HMC sampler.\n",
    "    thetas = sample_theta_space(\n",
    "        X,\n",
    "        Y,\n",
    "        active_dimensions,\n",
    "        samples[\"msq\"][-1],\n",
    "        samples[\"lambda\"][-1],\n",
    "        samples[\"eta1\"][-1],\n",
    "        samples[\"xisq\"][-1],\n",
    "        hypers[\"c\"],\n",
    "        samples[\"sigma\"][-1],\n",
    "    )\n",
    "    print(\"Single posterior sample theta:\\n\", thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sml505",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
